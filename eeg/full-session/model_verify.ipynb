{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "utility-meaning",
   "metadata": {},
   "source": [
    "# Verify that SVC classification is using information from EEG features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capital-nursing",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satellite-saying",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "from get_feature_response import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exclusive-fence",
   "metadata": {},
   "source": [
    "From prediction_model notebook, we know that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "realistic-rough",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_record_len = 35"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minute-canvas",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arctic-pencil",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture \n",
    "start_time = time.time()\n",
    "\n",
    "raw_path = \"raw\"\n",
    "all_sessions = os.listdir(raw_path)\n",
    "\n",
    "loaded_raws = []\n",
    "\n",
    "for session in all_sessions:\n",
    "    session_path = os.path.join(raw_path, session)\n",
    "    raw = mne.io.Raw(session_path, preload=True)\n",
    "    \n",
    "    loaded_raws.append(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "large-resolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Took %ss to finish.\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stunning-garlic",
   "metadata": {},
   "source": [
    "### Get emotion response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satellite-jewel",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"/net2/expData/affective_eeg/mahnob_dataset/Sessions\"\n",
    "meta_data_path = \"session.xml\"\n",
    "all_session_nums = os.listdir(dataset_path) # List of all session names\n",
    "\n",
    "session_nums = [] # Sessions with bdf recordings\n",
    "\n",
    "# Get current working directory to change back later\n",
    "curr_dir = os.getcwd()\n",
    "\n",
    "# From data manual, bdf file may not exist if \"the trials is missing due to \n",
    "# technical difficulties\" (pg 15).\n",
    "# Skip all sessions with no bdf recordings\n",
    "for session in all_session_nums:\n",
    "    session_path = os.path.join(dataset_path, session)\n",
    "    os.chdir(session_path)\n",
    "    bdf_list = glob.glob(\"*.bdf\")\n",
    "    \n",
    "    if (len(bdf_list) == 1):\n",
    "        session_nums.append(session)\n",
    "    \n",
    "    elif (len(bdf_list) > 1):\n",
    "        raise ValueError(\"Cannot handle multiple bdf files in one session.\")\n",
    "\n",
    "# Change back to notebook directory as a precaution\n",
    "os.chdir(curr_dir)\n",
    "print(\"Back to directory: \", os.getcwd())\n",
    "\n",
    "response_list = []\n",
    "\n",
    "for session in session_nums:\n",
    "    xml_path = os.path.join(dataset_path, session, meta_data_path)\n",
    "    resp = get_affect(xml_path, cutoff=5)\n",
    "    response_list.append(resp)\n",
    "\n",
    "response_array = np.array(response_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "waiting-danger",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"response_array shape:\", np.shape(response_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empty-joining",
   "metadata": {},
   "source": [
    "## Check SVC classification accuracy using irrelevant data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gross-narrow",
   "metadata": {},
   "source": [
    "### Use occipital lobe data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elementary-tribute",
   "metadata": {},
   "outputs": [],
   "source": [
    "FREQ_BANDS = [\"alpha\", \"beta\"]\n",
    "CHANNELS = [\"PO3\", \"O1\", \"Oz\", \"O2\", \"PO4\"]\n",
    "WINDOW = 2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fleet-assurance",
   "metadata": {},
   "source": [
    "__Warning: Long run time.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eastern-manhattan",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "start_time = time.time()\n",
    "\n",
    "features_list = []\n",
    "features_red_dim_list = []\n",
    "\n",
    "for raw in loaded_raws:\n",
    "    _features, _features_red_dim = bdf_to_features(raw=raw, \n",
    "                                                   duration=min_record_len,\n",
    "                                                   freq_bands=FREQ_BANDS, \n",
    "                                                   chs=CHANNELS,\n",
    "                                                   window=WINDOW)\n",
    "    features_list.append(_features)\n",
    "    features_red_dim_list.append(_features_red_dim)\n",
    "\n",
    "features_array = np.array(features_list)\n",
    "features_red_dim_array = np.array(features_red_dim_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aboriginal-baking",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Took %ss to finish.\" % (time.time() - start_time))\n",
    "print(\"features_list shape:\", np.shape(features_list))\n",
    "print(\"features_red_dim_list shape:\", np.shape(features_red_dim_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subtle-observer",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_v = make_pipeline(StandardScaler(), SVC())\n",
    "svc_v_1_scores = cross_val_score(svc_v, features_red_dim_list, response_array[:,0], cv=5)\n",
    "print(svc_v_1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-stylus",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "svc_a = make_pipeline(StandardScaler(), SVC())\n",
    "svc_a_1_scores = cross_val_score(svc_a, features_red_dim_list, response_array[:,1], cv=5)\n",
    "print(svc_a_1_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collected-southeast",
   "metadata": {},
   "source": [
    "## Check SVC classification accuracy using random data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorrect-international",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_data = np.random.uniform(0, 100, (527, 140))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-demonstration",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_v = SVC()\n",
    "svc_v_2_scores = cross_val_score(svc_v, random_data, response_array[:,0], cv=5)\n",
    "print(svc_v_2_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defensive-italian",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_a = SVC()\n",
    "svc_a_2_scores = cross_val_score(svc_a, random_data, response_array[:,1], cv=5)\n",
    "print(svc_a_1_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smooth-puzzle",
   "metadata": {},
   "source": [
    "__So it appears that SVC is not making classifications properly__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blessed-burton",
   "metadata": {},
   "source": [
    "## Check LDA accuracy using random data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latter-prison",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_v = LDA()\n",
    "lda_v_1_scores = cross_val_score(lda_v, random_data, response_array[:,0], cv=5)\n",
    "print(lda_v_1_scores)\n",
    "print(\"Mean CV score: \", np.mean(lda_v_1_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thousand-munich",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_a = LDA()\n",
    "lda_a_1_scores = cross_val_score(lda_a, random_data, response_array[:,0], cv=5)\n",
    "print(lda_a_1_scores)\n",
    "print(\"Mean CV score: \", np.mean(lda_a_1_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effective-congress",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_data = np.random.uniform(0, 100, (527, 140))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civic-billy",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_v = LDA()\n",
    "lda_v_1_scores = cross_val_score(lda_v, random_data, response_array[:,0], cv=5)\n",
    "print(lda_v_1_scores)\n",
    "print(\"Mean CV score: \", np.mean(lda_v_1_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changing-baptist",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_a = LDA()\n",
    "lda_a_1_scores = cross_val_score(lda_a, random_data, response_array[:,0], cv=5)\n",
    "print(lda_a_1_scores)\n",
    "print(\"Mean CV score: \", np.mean(lda_a_1_scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
